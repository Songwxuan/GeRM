<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>GeRM</title>

    <meta name="description" content="GeRM">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://qtransformer.github.io/img/qt_title_overview.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://qtransformer.github.io/">
    <meta property="og:title" content="GeRM">
    <meta property="og:description" content="Project page for GeRM.">

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="GeRM">
    <meta name="twitter:description" content="Project page for GeRM.">
    <meta name="twitter:image" content="https://qtransformer.github.io/img/qt_title_overview.png">


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="./germ_files/bootstrap.min.css">
    <link rel="stylesheet" href="./germ_files/font-awesome.min.css">
    <link rel="stylesheet" href="./germ_files/codemirror.min.css">
    <link rel="stylesheet" href="./germ_files/app.css">

    <link rel="stylesheet" href="./germ_files/bootstrap(1).min.css">

    <script src="./germ_files/jquery.min.js.下载"></script>
    <script src="./germ_files/bootstrap.min.js.下载"></script>
    <script src="./germ_files/codemirror.min.js.下载"></script>
    <script src="./germ_files/clipboard.min.js.下载"></script>
    
    <script src="./germ_files/app.js.下载"></script>
<!-- Google tag (gtag.js) -->
<script async="" src="./germ_files/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body data-new-gr-c-s-check-loaded="14.1163.0" data-gr-ext-installed="">
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">GeRM: </font></strong> <br> A <span style="font-weight: bold;">Ge</span>neralist <span style="font-weight: bold;">R</span>obotic <span style="font-weight: bold;">M</span>odel with Mixture-of-experts for Quadruped Robots <br> 
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li>Wenxuan Song</li> <li>Han Zhao</li> <li>Pengxiang Ding</li> <li>Can Cui</li> <li>Shangke Lyu</li> <li>Yaning Fan</li>  <li>Donglin Wang*</li>
                <br>
                <i>*corresponding author</i>
                <br>
                <span style="font-size: larger; font-weight: bold;">MiLAB, Westlake University, China</span>
		<br>
		<span style="font-size: larger; font-weight: bold;">IROS2024</span>
           <br>
                    
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2403.13358.pdf">
                            <img src="./germ_files/paper_small.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <li>
                            <a href="https://youtu.be/tjgIxsXW0JU">
                            <img src="./germ_files/youtube_icon_dark.png" height="60px">
                                <h4><strong>Videos</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
		<!-- <p style="text-align:center;">
	    		<img src="./germ_files/qt_animation.gif" class="img-responsive" style="width:100%">
		</p> -->
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Multi-task robot learning holds significant importance in tackling diverse and complex scenarios. However, current approaches are hindered by performance issues an difficulties in collecting training datasets. In this paper, we propose GeRM (Generalist Robotic Model). We utilize offline reinforcement learning to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations. Thereafter, we employ a transformer-based VLA network to process multimodal inputs and output actions. By introducing the Mixtureof-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multitask learning while controlling computational costs. Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also validating its efficiency in both training and inference processes. Additionally, we uncover its potential to acquire emergent skills. Additionally, we contribute the QUARD-Auto dataset, collected automatically to support our training approach and foster advancements in multi-task quadruped robot learning. This work presents a new paradigm for reducing the cost of collecting robot data and driving progress in the multi-task learning community.                </p>
            </div>
        </div>

<div class="row">
   <div class="col-md-8 col-md-offset-2">
	<h3>
	    Approach
	</h3>
    <p class="text-justify">
        GeRM takes both demonstration and sub-optimal data as input. Then the images and instructions are tokenized and sent into the GeRM mixture-of-experts Transformer Decoder to generate action tokens. They are finally de-tokenized into discretized robot commands. The actions are used for RL objectives when training.
        </p>
        <p style="text-align:center;">
	    <img src="./germ_files/teaser.png" class="img-responsive" style="width:100%">
	</p>
	<!-- <p style="text-align:center;">
		<img src="./germ_files/qt_robot_frames.png" class="img-responsive" style="width:70%">
        </p> -->
	<p class="text-justify">
	GeRM MoE Decoder is a
    transformer decoder-only model where the Feed-Forward
    Network (FFN) picks from a set of 8 distinct groups of
    parameters. At every layer, for every token, the gating network
    chooses two of the “experts to process the token and combine their output additively. Different experts
    are proficient in different tasks/different action dimensions to
    solve problems in different scenarios, learning a generalist
    model across multiple tasks. This technique increases the
    network parameter volume while keeping the computational
    cost basically unchanged.
	</p><p style="text-align:center;">
	        <img src="./germ_files/moe.png" class="img-responsive" style="width:75%">
	</p>
    <p>
        We utilize a vision-language-action (VLA) model to generate the robot action tokens, and then detokenize them into 12-dimension commands by low-level control policy.
    </p>
    <p style="text-align:center;">
            <img src="./germ_files/formula.png" class="img-responsive" style="width:75%">
    </p> 
    <p>
        The 12-dimension commands include:
    </p>   
    </p>
        <p style="text-align:center;">
                <img src="./germ_files/parameter.png" class="img-responsive" style="width:75%">
        </p>
        <p class="text-justify">
            Finally, we utilize the effective offline RL algorithm - Deep Q-Learning to learn the optimal policy from mix-quality data.
            Specifically, we adopt designs akin to 
            <a href="https://arxiv.org/abs/2309.10150">GeRM</a>
            by employing the VLA model to replace the value function and output discretized action.
        </p>
    
    </div>
</div>
	    
<div class="row">
<div class="col-md-8 col-md-offset-2">
    <h3>
        Datasets
    </h3>
<p class="text-justify">
We proposed an autonomous diagram to collect robot multi-modal data. In this way, We construct a large-scale robot dataset - <span style="font-weight: bold;"> QUARD-Auto</span>, with a combination of demonstrations and sub-optimal data. It includes 5 tasks and 99 sub-tasks, totally <span style="font-weight: bold;"> 257k</span> episodes (trajectories).
This dataset will be accessible several months subsequent to its organization. Kindly visit our 
<a href="https://github.com/Songwxuan/GeRM">GitHub page </a>
to obtain the dataset at that time.
</p>
<p style="text-align:center;">
        <img src="./germ_files/task.png" class="img-responsive">
</p>
<p style="text-align:center;">
    <img src="./germ_files/dataset.png" class="img-responsive" style="width:65%">
</p>
</div>
</div>

<div class="row" id="videos">
    <div class="col-md-8 col-md-offset-2">
	<h3>
	    Results and Videos
	</h3>
	<p class="text-justify">
        We conducted a comprehensive and robust series of experiments. To ensure data fidelity and mitigate the impact of stochastic variability, our primary experiments for each model encompassed the entirety of tasks including all 99 sub-tasks, with 400 trajectories meticulously tested for each.
        </p><p style="text-align:center;">
	    <img src="./germ_files/exp.png" class="img-responsive" style="width:100%">
	</p>
    <p>
    GeRM has the <span style="font-weight: bold;"> highest success rate</span> among all tasks. Compared to such baselines as <a href="https://arxiv.org/abs/2212.06817">RT-1</a> and other varieties of GeRM, it effectively learns from mix-quality data, outperforms other methods, and demonstrates superior capabilities in multi-tasks.
    Meanwhile, MoE Modules balance <span style="font-weight: bold;">computational cost</span> and performance by activating part of the parameterwhen inference.
	</p>
	
	<p class="text-justify"> 
        GeRM exhibits commendable training efficiency. 
        Compared to other methods, Germ achieves extremely low loss and high success rates with only a few batches. 
        This evaluation exclusively trained with demonstrations underscores GeRM's proficiency in <span style="font-weight: bold;"> optimizing data utilization strategies</span>.
	</p>
	  <p style="text-align:center;">
	    <img src="./germ_files/trend.png" class="img-responsive" style="width:100%">
	</p>
    <p>
        Solid lines represent the success rate, dotted lines represent the final success rate for 20 epochs, and dashed lines represent loss. Note: RL approaches employ MSE loss, which should be scaled by 0.1, while imitation learning ways employ Cross Entropy as the loss function.
    </p>
    <p>
        <div style="text-align: center;">
        <iframe width="640" height="360" src="https://www.youtube.com/embed/tjgIxsXW0JU?si=ig-IGGukGbKrE-gq" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
	</p>
    <p>
        GeRM shows <span style="font-weight: bold;">emergent skills </span>in dynamic adaptive path planning.
        As shown in this video, the quadruped robot's vision is limited at the initial position, hampering its ability to
        determine the direction of movement. To avoid the obstacle it
        turns to the left randomly. Subsequently, upon encountering
        the incorrect visual input, the robot executes a substantial
        reorientation to align with the correct target outside its
        original field of view. It then proceeds to steer towards the
        destination, ultimately accomplishing the task. Notably, such
        trajectories were out-of-distribution of our training dataset.
        This demonstrates that GeRM inherits novel capabilities in terms of
        <span style="font-weight: bold;">dynamic adaptive path planning </span>in the context of the scene,
        which means it can make decisions, plan future paths, and
        change next-step action according to the visual perception.
    </p>
    
         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly="" style="display: none;">@inproceedings{Song2024GeRMAG,title={GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot},author={Wenxuan Song and Han Zhao and Pengxiang Ding and Can Cui and Shangke Lyu and Yaning Fan and Donglin Wang},year={2024},url={https://arxiv.org/abs/2403.13358}}
                    </textarea>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p class="text-justify">
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>

</div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>
