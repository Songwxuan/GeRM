<!DOCTYPE html>
<!-- saved from url=(0031)https://qtransformer.github.io/ -->
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Q-Transformer</title>

    <meta name="description" content="Q-Transformer">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://qtransformer.github.io/img/qt_title_overview.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://qtransformer.github.io/">
    <meta property="og:title" content="Q-Transformer">
    <meta property="og:description" content="Project page for Q-Transformer.">

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Q-Transformer">
    <meta name="twitter:description" content="Project page for Q-Transformer.">
    <meta name="twitter:image" content="https://qtransformer.github.io/img/qt_title_overview.png">


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="./Q-Transformer_files/bootstrap.min.css">
    <link rel="stylesheet" href="./Q-Transformer_files/font-awesome.min.css">
    <link rel="stylesheet" href="./Q-Transformer_files/codemirror.min.css">
    <link rel="stylesheet" href="./Q-Transformer_files/app.css">

    <link rel="stylesheet" href="./Q-Transformer_files/bootstrap(1).min.css">

    <script src="./Q-Transformer_files/jquery.min.js.下载"></script>
    <script src="./Q-Transformer_files/bootstrap.min.js.下载"></script>
    <script src="./Q-Transformer_files/codemirror.min.js.下载"></script>
    <script src="./Q-Transformer_files/clipboard.min.js.下载"></script>
    
    <script src="./Q-Transformer_files/app.js.下载"></script>
<!-- Google tag (gtag.js) -->
<script async="" src="./Q-Transformer_files/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body data-new-gr-c-s-check-loaded="14.1163.0" data-gr-ext-installed="">
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">Q-Transformer: </font></strong> <br> Scalable Offline Reinforcement Learning via Autoregressive Q-Functions <br> 
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li>Yevgen Chebotar*</li> <li>Quan Vuong*</li> <li>Alex Irpan</li> <li>Karol Hausman</li> <li>Fei Xia</li> <li>Yao Lu</li>  <li>Aviral Kumar</li> <br>
                 <li>Tianhe Yu</li> <li>Alexander Herzog</li> <li>Karl Pertsch</li> <li>Keerthana Gopalakrishnan</li> <li>Julian Ibarz</li> <li>Ofir Nachum</li>  <br>
                  <li>Sumedh Sontakke</li> <li>Grecia Salazar</li> <li>Huong T Tran</li> <li>Jodilyn Peralta</li> <li>Clayton Tan</li> <li>Deeksha Manjunath</li> <br>
                  <li>Jaspiar Singht</li> <li>Brianna Zitkovich</li> <li>Tomas Jackson</li>  <li>Kanishka Rao</li> <li>Chelsea Finn</li> <li>Sergey Levine</li>
                <br>
		   <i>*equal contribution</i>
		<br>
                    <a href="http://deepmind.com/">
                    <img src="./Q-Transformer_files/deepmind.png" height="50px"> </a>
                    
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://qtransformer.github.io/assets/qtransformer.pdf">
                            <img src="./Q-Transformer_files/paper_small.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <li>
                            <a href="https://qtransformer.github.io/#videos">
                            <img src="./Q-Transformer_files/youtube_icon_dark.png" height="60px">
                                <h4><strong>Videos</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
		<p style="text-align:center;">
	    		<img src="./Q-Transformer_files/qt_animation.gif" class="img-responsive" style="width:100%">
		</p>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite.
                </p>
            </div>
        </div>

	<!--
	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/UuKAp9a6wMs" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
	-->

<div class="row">
   <div class="col-md-8 col-md-offset-2">
	<h3>
	    Approach
	</h3>

        <p style="text-align:center;">
	    <img src="./Q-Transformer_files/qt_title_overview.png" class="img-responsive" style="width:65%">
	</p>
	<p style="text-align:center;">
		<img src="./Q-Transformer_files/qt_robot_frames.png" class="img-responsive" style="width:70%">
        </p>
	<p class="text-justify">
	We first describe how to enable using Transformers for Q-learning by applying discretization and autoregression of the action space.
	The classical way for learning a Q-function using TD-learning is based on the Bellman update rule:
	</p><p style="text-align:center;">
	        <img src="./Q-Transformer_files/bellman_original.png" class="img-responsive" style="width:55%">
	</p>
	<p class="text-justify">
	We change the Bellman update to be performed for each action dimension by transforming the original MDP of the problem into an MDP where each
	action dimension is treated as a separate step for Q-learning. In particular, given the action dimensionality <i>d<sub>A</sub></i>, the new Bellman update rule is:
	</p>
	<p style="text-align:center;">
	        <img src="./Q-Transformer_files/bellman_qt.png" class="img-responsive">
	</p>
	<p>
	This means that for each intermediate action dimension we maximize over the next action dimension given the same state, 
	and for the final action dimension we use the first action dimension from the next state. This decomposition makes sure that the maximization 
	within the Bellman update remains tractable while ensuring that we still solve the original MDP problem.
	</p>
		
	<p style="text-align:center;">
	        <img src="./Q-Transformer_files/qt_update_fig.png" class="img-responsive">
	</p>
	<p>
	In order to account for the distribution shift during offline learning, we introduce a simple regularization technique 
	that minimizes unseen actions (in the discretized case unseen action bins) to the lowest value. To accelerate learning, we also employ
	Monte-Carlo (MC) returns that use the original return-to-go from a given episode and n-step returns that can skip per-dimension maximization.
	</p>	    
 </div>
</div>
	    
<div class="row" id="videos">
    <div class="col-md-8 col-md-offset-2">
	<h3>
	    Results and Videos
	</h3>
	<p class="text-justify">
        In our experiments, we start by evaluating Q-Transformer on a suite of real world tasks introduced in the RT-1 paper while limiting the data per task to only contain 100 human demonstrations. In addition to demonstrations, we also add autonomously collected failed episodes, resulting in a dataset of 38,000 positive examples from demos and 20,000 negative autonomously collected examples.
        </p><p style="text-align:center;">
	    <img src="./Q-Transformer_files/real_results_table.png" class="img-responsive" style="width:65%">
	</p>
	<p>
	    <video id="qt-robot-videos" autoplay="" loop="" muted="" playsinline="" controls="" style="width:100%">
       		 <source src="videos/qt_robot_videos.mp4" type="video/mp4">
        	Your browser does not support the video tag.
   	    </video>
	</p>

		    
    Compared to such baselines as <a href="https://arxiv.org/abs/2212.06817">RT-1</a>,
    <a href="https://arxiv.org/abs/2110.06169">IQL</a> and
    <a href="https://arxiv.org/abs/2106.01345">Decision Transformer (DT)</a>, Q-Transformer can effectively utilize autonomous episodes to significantly improve on such skills as picking from and placing objects into drawers, moving objects near targets and closing and opening drawers.
	improve on such skills as picking from and placing objects into drawers, moving objects near targets and closing and opening drawers.
	<p></p>

	<p class="text-justify">
	We also benchmark our method in a challenging simulated picking task, where only ~8% of the data are positive examples, and the rest are
	noisy negative examples. Q-learning methods, such as 
    <a href="https://arxiv.org/abs/1806.10293">QT-Opt</a>
    , 
    <a href="https://arxiv.org/abs/2110.06169">IQL</a>
    ,
    <a href="https://arxiv.org/abs/2111.05424">AW-Opt</a>
     and our Q-Transformer are generally performing better on this task
	as they are able to utilize negative examples to learn policies through dynamic programming.
	</p>
	<p style="text-align:center;">
	    <img src="./Q-Transformer_files/sim_exps.png" class="img-responsive" style="width:70%">
	</p>
	
	<p class="text-justify">
        Ablating our decision choices on this picking task, we notice that both the conservative regularizer and MC returns are important for retaining the performance. Switching to a Softmax regularizer, which is similar to a 
        <a href="https://arxiv.org/abs/2006.04779">CQL</a>
         regularizer for discrete actions, performs significantly worse as it bounds the policy too much to the distribution of the data, showing that our choice of the regularizer works better for such tasks.
	</p>
	  <p style="text-align:center;">
	    <img src="./Q-Transformer_files/qt_ablations.png" class="img-responsive" style="width:70%">
	</p>
		  
	<p> 
	 We also ablate n-step returns and notice that although introducing bias they can help us achieving the same high performance in much fewer number of gradient steps,
	making them an efficient choice in many problems.
	</p>
	 <p style="text-align:center;">
	    <img src="./Q-Transformer_files/nstep_table.png" class="img-responsive" style="width:55%">
	</p>
		  
	<p class="text-justify">
        We also try to run our Q-Transformer on a much larger dataset, scaling up the number of positive examples to 115,000 and the number of negative examples to 185,000 resulting in 300,000 episodes. Q-Transformer is still able to learn from this large dataset and even provide some improvement over the RT-1 BC baseline.
	</p>
		
	 <p style="text-align:center;">
	    <img src="./Q-Transformer_files/large_offline_results.png" class="img-responsive" style="width:43%">
	</p>

	<p class="text-justify">
	Finally, we use the Q-function trained by Q-Transformer as an affordance model in combination with a language planner,
	similar to the 
    <a href="https://arxiv.org/abs/2204.01691">SayCan</a>
    work.
	</p>
	 <p style="text-align:center;">
	    <img src="./Q-Transformer_files/affordance_perf.png" class="img-responsive" style="width:55%">
	</p>
	<p>
        Q-Transformer affordance estimation works better than the previously used Q-functions trained with 
        QT-Opt
        , especially when combined with relabeling non-sampled tasks as negatives for the current task during training. As Q-Transformer does not require sim-to-real training that was used for the 
        QT-Opt
        training, it makes it easier to use it in the absence of suitable simulations.
	</p>

	<p class="text-justify">
        To test the full planning + execution system, we use Q-Transformer for both affordance estimation and the actual policy execution, where it shows to outperform the previous combination of 
        QT-Opt
         and 
        RT-1.
	</p>
	 <p style="text-align:center;">
	    <img src="./Q-Transformer_files/saycan_perf.png" class="img-responsive" style="width:55%">
	</p>
	<p>
        </p>
	<p>
	    <video id="qt-saycan-videos" autoplay="" loop="" muted="" playsinline="" controls="" style="width:100%">
       		 <source src="videos/qt_saycan.mp4" type="video/mp4">
        	Your browser does not support the video tag.
   	    </video>
	</p>
    As can be seen in the examples of task affordance values for a given image, Q-Transformer can provide high-quality affordance values that can be used in downstream plan-and-execute frameworks.
</div>
</div>

<!--
<div class="row">
    <div class="col-md-8 col-md-offset-2">
	<h3>
	    Video
	</h3>
	<video id="v0" width="100%" playsinline muted loop controls>
	       <source src="img/Q-Transformer_Video.mov" type="video/mp4">
        </video>		
    </div>
</div>
-->
         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly="" style="display: none;">@inproceedings{qtransformer,
    title={Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions},
    authors={Yevgen Chebotar and Quan Vuong and Alex Irpan and Karol Hausman and Fei Xia and Yao Lu and Aviral Kumar and Tianhe Yu and Alexander Herzog and Karl Pertsch and Keerthana Gopalakrishnan and Julian Ibarz and Ofir Nachum and Sumedh Sontakke and Grecia Salazar and Huong T Tran and Jodilyn Peralta and Clayton Tan and Deeksha Manjunath and Jaspiar Singht and Brianna Zitkovich and Tomas Jackson and Kanishka Rao and Chelsea Finn and Sergey Levine},
    booktitle={7th Annual Conference on Robot Learning},
    year={2023}}</textarea><div class="CodeMirror cm-s-default CodeMirror-wrap"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 4px; left: 4px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" style="position: absolute; padding: 0px; width: 1000px; height: 1em; outline: none;" tabindex="0"></textarea></div><div class="CodeMirror-vscrollbar" cm-not-content="true"><div style="min-width: 1px; height: 0px;"></div></div><div class="CodeMirror-hscrollbar" cm-not-content="true"><div style="height: 100%; min-height: 1px; width: 0px;"></div></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: -17px; border-right-width: 13px; min-height: 197px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines"><div style="position: relative; outline: none;"><div class="CodeMirror-measure">AخA</div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-cursors"><div class="CodeMirror-cursor" style="left: 4px; top: 0px; height: 17.1375px;">&nbsp;</div></div><div class="CodeMirror-code" style=""><pre class=" CodeMirror-line "><span style="padding-right: 0.1px;">@inproceedings{qtransformer,</span></pre><pre class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  title={Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions},</span></pre><pre class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  authors={Yevgen Chebotar and Quan Vuong and Alex Irpan and Karol Hausman and Fei Xia and Yao Lu and Aviral Kumar and Tianhe Yu and Alexander Herzog and Karl Pertsch and Keerthana Gopalakrishnan and Julian Ibarz and Ofir Nachum and Sumedh Sontakke and Grecia Salazar and Huong T Tran and Jodilyn Peralta and Clayton Tan and Deeksha Manjunath and Jaspiar Singht and Brianna Zitkovich and Tomas Jackson and Kanishka Rao and Chelsea Finn and Sergey Levine},</span></pre><pre class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  booktitle={7th Annual Conference on Robot Learning},</span></pre><pre class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  year={2023}}</span></pre></div></div></div></div></div><div style="position: absolute; height: 13px; width: 1px; top: 197px;"></div><div class="CodeMirror-gutters" style="display: none; height: 210px;"></div></div></div>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p class="text-justify">
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
<!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
The authors would like to acknowledge TODO and the greater teams at Google DeepMind for their feedback and contributions.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
	    -->


</div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>